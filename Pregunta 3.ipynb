{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 4\n",
    "\n",
    "### Funciones de activaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def softmax_prime(x):\n",
    "    upper = np.exp(-x)\n",
    "    down = np.power(1 + np.exp(-x), 2)\n",
    "    return upper / down\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return np.exp(-x)/((1+np.exp(-x))**2)\n",
    "\n",
    "def relu(x):\n",
    "    return np.abs(x) * (x > 0)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Convert the targets to one hot vectors\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed with a score of:  0.36\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e1b4676f33e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnumgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputeGradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e1b4676f33e7>\u001b[0m in \u001b[0;36mcomputeGradients\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcomputeGradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mdJdW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdJdW2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdJdW2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e1b4676f33e7>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0myHat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myHat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_derivative\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randint, random, seed, uniform\n",
    "import keras\n",
    "    \n",
    "class Network:\n",
    "    # The layers variable receives a specification with the following format:\n",
    "    # [\n",
    "    #    \"neurons\": n_neurones,\n",
    "    #    \"activation\": \"activation_function\",\n",
    "    # ]\n",
    "    \n",
    "    activation_function_hash = {\n",
    "        \"softmax\": {\n",
    "            \"func\": softmax,\n",
    "            \"func_prime\": softmax_prime,\n",
    "        },\n",
    "        \"sigmoid\": {\n",
    "            \"func\": sigmoid,\n",
    "            \"func_prime\": sigmoid_prime,\n",
    "        },\n",
    "        \n",
    "        \"relu\": {\n",
    "            \"func\": relu,\n",
    "            \"func_prime\": relu_prime,\n",
    "        },\n",
    "        \n",
    "        \"linear\": {\n",
    "            \"func\": linear,\n",
    "            \"func_prime\": 1,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # loss defines the method to calculate cost\n",
    "    # crossentropy or mse\n",
    "    def __init__(self, layers, loss=\"mse\"):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        \n",
    "        # The first layer is the input layer, and does not have biases\n",
    "        # or weights\n",
    "        self.biases = np.array([np.random.randn(l[\"neurons\"], 1) for l in layers[1:]])\n",
    "        \n",
    "        # The amount of weights depensd on both the amount of neurones on the layer \n",
    "        # and the dimension of the inputs.\n",
    "        self.weights = [np.random.randn(curr_l[\"neurons\"], prev_l[\"neurons\"]) for prev_l, curr_l in zip(layers[:-1], layers[1:])]\n",
    "\n",
    "    # Returns final output, activation output at each layer\n",
    "    # and z value at each layer\n",
    "    def forward_propagation(self, data):\n",
    "        output = data\n",
    "        activations = [data.reshape(len(data), 1)]\n",
    "        zs = []\n",
    "        self.weights_list = []\n",
    "        # We start on the first layer. Note that the input layer\n",
    "        # is pretty much ignored as it does not have activation\n",
    "        # functions or anything like that.\n",
    "        for B, W, layer, index in zip(self.biases, self.weights, self.layers, range(0, len(self.biases))):\n",
    "            dot = np.dot(W, output)\n",
    "            self.weights_list.append(W)\n",
    "            dot = dot.reshape(len(dot), 1)\n",
    "            Z = dot + B\n",
    "\n",
    "            zs.append(Z)\n",
    "            activation_string = layer[\"activation\"]\n",
    "            g = Network.activation_function_hash[activation_string][\"func\"]\n",
    "            output = g(Z)\n",
    "            \n",
    "            \n",
    "            \n",
    "            activations.append(output)\n",
    "\n",
    "        return activations, zs    \n",
    "    \n",
    "    \n",
    "    # Devuelve el output de la red\n",
    "    def evaluate(self, X):\n",
    "        output = []\n",
    "        for x in X:\n",
    "            (acts, zs) = self.forward_propagation(x)\n",
    "            output.append(self.classify(acts[len(self.biases)]))\n",
    "            \n",
    "        return np.array(output)\n",
    "    \n",
    "    def classify(self, x):\n",
    "        chosen_class = 2 ** np.argmax(x)\n",
    "        total_classes = self.layers[-1][\"neurons\"]\n",
    "        return (((chosen_class & (1 << np.arange(total_classes)))) > 0).astype(int)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Receives a single sample X with its \n",
    "    # corresponding value Y and returns the \n",
    "    # derivatives for the weights and biases\n",
    "    # We want to get dc/db and dc/dw\n",
    "    \n",
    "    def get_total_cost(self):\n",
    "        if self.loss == \"mse\":\n",
    "            return 0.5*np.linalg.norm(a-y)**2\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "        return None\n",
    "    \n",
    "    def get_delta(self, a, z, y, activation_derivative):\n",
    "        # Equation 1 says how this should got, at least for mse\n",
    "        if self.loss == \"mse\":\n",
    "            return (a - y) * activation_derivative(z)\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            return (a - y)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        \n",
    "        # Initialize the gradient\n",
    "        gradient_b = np.array([np.zeros(bias.shape) for bias in self.biases])\n",
    "        gradient_w = np.array([np.zeros(weight.shape) for weight in self.weights])\n",
    "        \n",
    "        # Propagate the input through the network and get all the\n",
    "        # z outputs and activation outputs of all layers\n",
    "        # The first activation corresponds to the first input layer\n",
    "        (acts, zs)  = self.forward_propagation(x)\n",
    "        \n",
    "        # Use the specified activation function's derivative\n",
    "        activation_derivative = Network.activation_function_hash[self.layers[-1][\"activation\"]][\"func_prime\"]\n",
    "        \n",
    "        # We first calculate the delta value of the output layer, given as\n",
    "        # delta_cost = aL - y\n",
    "        # [ , , , , X] (= -1)\n",
    "        # [ , , , X, ] ( = -2)\n",
    "        # ....\n",
    "        # [X, , , , ,] ( = -L)\n",
    "        \n",
    "        yHat = np.array(self.classify(acts[-1]))\n",
    "\n",
    "        delta = self.get_delta(yHat.reshape(3, 1), zs[-1], y.reshape(3, 1), activation_derivative)\n",
    "        \n",
    "        # \n",
    "        # Gradient B = delta\n",
    "        gradient_b[-1] = delta\n",
    "        \n",
    "        # Gradient W = delta x A(L - 1)T\n",
    "        gradient_w[-1] = np.dot(delta, acts[-2].T)\n",
    "        \n",
    "        # We now have the L layer with its deltas and gradients claculated.\n",
    "        # Now we iterate over each layer to get the specific deltas\n",
    "        for l in range(2, len(self.layers)):\n",
    "            z = zs[-l] # The minus syntax makes the array to get the farther\n",
    "            \n",
    "            # Use the specified activation function's derivative\n",
    "            activation_derivative = Network.activation_function_hash[self.layers[-l][\"activation\"]][\"func_prime\"]\n",
    "            \n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * activation_derivative(z)\n",
    "            \n",
    "            gradient_b[-l] = delta\n",
    "            \n",
    "            gradient_w[-l] = np.dot(delta, acts[-l - 1].T)\n",
    "            \n",
    "        return (gradient_b, gradient_w)\n",
    "            \n",
    "    def costFunction(self,X,Y):\n",
    "        n = self.layers[-1][\"neurons\"]\n",
    "        \n",
    "        cost = 0.0\n",
    "        A = self.evaluate(X)     \n",
    "        if self.loss == \"mse\":\n",
    "            for a, y in zip(A, Y):\n",
    "                cost += 0.5*np.linalg.norm(a.reshape(n, 1) - np.array(y).reshape(n, 1))**2 / len(A)\n",
    "            return cost\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            for a, y in zip(A, Y):\n",
    "                a = a.reshape(n, 1)\n",
    "                y = np.array(y.reshape(n, 1))\n",
    "                cost += np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) / len(A)\n",
    "        return cost\n",
    "    def getParams(self):\n",
    "        \n",
    "        return self.weights.ravel()\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        \n",
    "        self.weights = params\n",
    "        \n",
    "        \n",
    "    def computeGradients(self, x, y):\n",
    "        dJdW1, dJdW2 = self.backprop(x, y)\n",
    "        return dJdW2.ravel()\n",
    "    \n",
    "    def SGD(self, trainX, trainY, epochs, learn_rate):\n",
    "        \n",
    "        change_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        change_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "    \n",
    "        for epoch in range(0, epochs):\n",
    "            # print(\"Epoch\", epoch + 1)\n",
    "            for X, Y in zip(trainX, trainY):\n",
    "\n",
    "                \"\"\"\n",
    "                The backprop function will propagate the current information\n",
    "                through the network and then calculate the gradients of\n",
    "                each layer. Considering that we have a single bias per layer,\n",
    "                gradient_b will be an array of size L, being L the total number\n",
    "                of layers, excluding the input layer.\n",
    "\n",
    "                gradient_w will be an array of size L too, in which each element\n",
    "                becomes a vector that represents the gradient of all the weights\n",
    "                of a layer.\n",
    "                \"\"\"\n",
    "                (gradient_b, gradient_w) = self.backprop(X, Y)\n",
    "\n",
    "                \"\"\"\n",
    "                Now that we have the gradients of the weights, we want to\n",
    "                store this information for this particular training example in\n",
    "                order to average it later on. The change_b represents the sum of\n",
    "                the changes of all training examples for all biases, and change_w\n",
    "                does the same for all weights.\n",
    "\n",
    "                Later, as if we were trying to get the average of a list, we will\n",
    "                divide this sum by the total amount of examples. This will tell\n",
    "                us the average change of all training examples and allow us to\n",
    "                modify the weights.\n",
    "                \"\"\"\n",
    "                change_b = [(cb + gb)\n",
    "                            for cb, gb in zip(change_b, gradient_b)]\n",
    "                change_w = [(cw + gw)\n",
    "                            for cw, gw in zip(change_w, gradient_w)]\n",
    "\n",
    "            self.weights = [weight - (learn_rate / len(trainX)) * weight_change for weight, weight_change in zip(self.weights, change_w)]\n",
    "            self.biases = [bias -(learn_rate / len(trainX))*bias_change for bias, bias_change in zip(self.biases, change_b)]    \n",
    "        \n",
    "                \n",
    "    \n",
    "# The network should have 2 layers, the first with 32 neurons and the second with 16\n",
    "# You basically will have an architecture like this:\n",
    "# input -> layer 1 (32) -> later 2 (16) -> output (3 neurons with softmax activation function)\n",
    "\n",
    "\"\"\"\n",
    "Bellow here we test our implementation\n",
    "\"\"\"\n",
    "import keras\n",
    "seed(10)\n",
    "T = 100\n",
    "M = 5\n",
    "dataX = np.array([np.random.randn(M, 1) for n in range(0, T)])\n",
    "dataY = [randint(0,2) for n in range(0, T)]\n",
    "dataYOnehot = keras.utils.to_categorical(dataY)\n",
    "network = Network([{\n",
    "    \"neurons\": M,\n",
    "    \"activation\": \"linear\"\n",
    "}, {\n",
    "    \"neurons\": 32,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 16,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 3,\n",
    "    \"activation\": \"softmax\"\n",
    "}])\n",
    "\n",
    "x = dataX[0]\n",
    "y = dataY[0]\n",
    "\n",
    "\n",
    "network.SGD(dataX, dataYOnehot, 300, 0.1)\n",
    "yHat = np.array(network.evaluate(dataX))\n",
    "\n",
    "# Measure accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Tests passed with a score of: \", accuracy_score(dataYOnehot, yHat))\n",
    "\n",
    "\n",
    "def computeNumericalGradient(N, x, y):\n",
    "        paramsInitial = N.getParams()\n",
    "        numgrad = np.zeros(paramsInitial.shape)\n",
    "        perturb = np.zeros(paramsInitial.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(paramsInitial)):\n",
    "            #Set perturbation vector\n",
    "            perturb[p] = e\n",
    "            N.setParams(paramsInitial + perturb)\n",
    "            loss2 = N.costFunction(X, y)\n",
    "            \n",
    "            N.setParams(paramsInitial - perturb)\n",
    "            loss1 = N.costFunction(X, y)\n",
    "\n",
    "            #Compute Numerical Gradient\n",
    "            numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "\n",
    "            #Return the value we changed to zero:\n",
    "            perturb[p] = 0\n",
    "            \n",
    "        #Return Params to original value:\n",
    "        N.setParams(paramsInitial)\n",
    "\n",
    "        return numgrad\n",
    "grad = network.computeGradients(x,y)\n",
    "grad\n",
    "\n",
    "numgrad = computeNumericalGradient(network, x, y)\n",
    "numgrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Network' object has no attribute 'get_costs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c902e1f3c830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_costs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Network' object has no attribute 'get_costs'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "costs = network.get_costs()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(0, len(costs)), costs, label=\"Loss\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
