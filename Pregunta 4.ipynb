{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 4\n",
    "\n",
    "### Funciones de activaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def softmax_prime(x):\n",
    "    upper = np.exp(-x)\n",
    "    down = np.power(1 + np.exp(-x), 2)\n",
    "    return upper / down\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return np.exp(-x)/((1+np.exp(-x))**2)\n",
    "\n",
    "def relu(x):\n",
    "    return np.abs(x) * (x > 0)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Convert the targets to one hot vectors\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Feed-forward con momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, random, seed, uniform\n",
    "import keras\n",
    "    \n",
    "class Network:\n",
    "    # The layers variable receives a specification with the following format:\n",
    "    # [\n",
    "    #    \"neurons\": n_neurones,\n",
    "    #    \"activation\": \"activation_function\",\n",
    "    # ]\n",
    "    \n",
    "    activation_function_hash = {\n",
    "        \"softmax\": {\n",
    "            \"func\": softmax,\n",
    "            \"func_prime\": softmax_prime,\n",
    "        },\n",
    "        \"sigmoid\": {\n",
    "            \"func\": sigmoid,\n",
    "            \"func_prime\": sigmoid_prime,\n",
    "        },\n",
    "        \n",
    "        \"relu\": {\n",
    "            \"func\": relu,\n",
    "            \"func_prime\": relu_prime,\n",
    "        },\n",
    "        \n",
    "        \"linear\": {\n",
    "            \"func\": linear,\n",
    "            \"func_prime\": 1,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # loss defines the method to calculate cost\n",
    "    # crossentropy or mse\n",
    "    def __init__(self, layers, loss=\"mse\", mu = 1.0):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.loss_history = []\n",
    "        self.mu = mu\n",
    "        \n",
    "        # The first layer is the input layer, and does not have biases\n",
    "        # or weights\n",
    "        self.biases = np.array([np.random.randn(l[\"neurons\"], 1) for l in layers[1:]])\n",
    "        \n",
    "        # The amount of weights depensd on both the amount of neurones on the layer \n",
    "        # and the dimension of the inputs.\n",
    "        self.weights = [np.random.randn(curr_l[\"neurons\"], prev_l[\"neurons\"]) for prev_l, curr_l in zip(layers[:-1], layers[1:])]\n",
    "        self.velocities = [np.zeros((curr_l[\"neurons\"], prev_l[\"neurons\"])) for prev_l, curr_l in zip(layers[:-1], layers[1:])]\n",
    "\n",
    "    # Returns final output, activation output at each layer\n",
    "    # and z value at each layer\n",
    "    def forward_propagation(self, data):\n",
    "        output = data\n",
    "        activations = [data.reshape(len(data), 1)]\n",
    "        zs = []\n",
    "        \n",
    "        # We start on the first layer. Note that the input layer\n",
    "        # is pretty much ignored as it does not have activation\n",
    "        # functions or anything like that.\n",
    "        for B, W, layer, index in zip(self.biases, self.weights, self.layers, range(0, len(self.biases))):\n",
    "            dot = np.dot(W, output)\n",
    "            dot = dot.reshape(len(dot), 1)\n",
    "            Z = dot + B\n",
    "\n",
    "            zs.append(Z)\n",
    "            activation_string = layer[\"activation\"]\n",
    "            g = Network.activation_function_hash[activation_string][\"func\"]\n",
    "            output = g(Z)\n",
    "            \n",
    "            \n",
    "            \n",
    "            activations.append(output)\n",
    "\n",
    "        return activations, zs    \n",
    "    \n",
    "    \n",
    "    # Devuelve el output de la red\n",
    "    def evaluate(self, X):\n",
    "        output = []\n",
    "        for x in X:\n",
    "            (acts, zs) = self.forward_propagation(x)\n",
    "            output.append(self.classify(acts[len(self.biases)]))\n",
    "            \n",
    "        return np.array(output)\n",
    "    \n",
    "    def classify(self, x):\n",
    "        chosen_class = 2 ** np.argmax(x)\n",
    "        total_classes = self.layers[-1][\"neurons\"]\n",
    "        return (((chosen_class & (1 << np.arange(total_classes)))) > 0).astype(int)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Receives a single sample X with its \n",
    "    # corresponding value Y and returns the \n",
    "    # derivatives for the weights and biases\n",
    "    # We want to get dc/db and dc/dw\n",
    "    \n",
    "    def get_total_cost(self, X, Y):\n",
    "        \n",
    "        n = self.layers[-1][\"neurons\"]\n",
    "        \n",
    "        cost = 0.0\n",
    "        A = self.evaluate(X)\n",
    "        \n",
    "        \n",
    "        if self.loss == \"mse\":\n",
    "            for a, y in zip(A, Y):\n",
    "                cost += 0.5*np.linalg.norm(a.reshape(n, 1) - np.array(y).reshape(n, 1))**2 / len(A)\n",
    "            return cost\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            for a, y in zip(A, Y):\n",
    "                a = a.reshape(n, 1)\n",
    "                y = np.array(y.reshape(n, 1))\n",
    "                cost += np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) / len(A)\n",
    "        return cost\n",
    "    def get_delta(self, a, z, y, activation_derivative):\n",
    "        # Equation 1 says how this should got, at least for mse\n",
    "        if self.loss == \"mse\":\n",
    "            return (a - y) * activation_derivative(z)\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            return (a - y)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        \n",
    "        # Initialize the gradient\n",
    "        gradient_b = np.array([np.zeros(bias.shape) for bias in self.biases])\n",
    "        gradient_w = np.array([np.zeros(weight.shape) for weight in self.weights])\n",
    "        \n",
    "        # Propagate the input through the network and get all the\n",
    "        # z outputs and activation outputs of all layers\n",
    "        # The first activation corresponds to the first input layer\n",
    "        (acts, zs)  = self.forward_propagation(x)\n",
    "        \n",
    "        # Use the specified activation function's derivative\n",
    "        activation_derivative = Network.activation_function_hash[self.layers[-1][\"activation\"]][\"func_prime\"]\n",
    "        \n",
    "        # We first calculate the delta value of the output layer, given as\n",
    "        # delta_cost = aL - y\n",
    "        # [ , , , , X] (= -1)\n",
    "        # [ , , , X, ] ( = -2)\n",
    "        # ....\n",
    "        # [X, , , , ,] ( = -L)\n",
    "        \n",
    "        yHat = np.array(self.classify(acts[-1]))\n",
    "\n",
    "        delta = self.get_delta(yHat.reshape(3, 1), zs[-1], y.reshape(3, 1), activation_derivative)\n",
    "        \n",
    "        # \n",
    "        # Gradient B = delta\n",
    "        gradient_b[-1] = delta\n",
    "        \n",
    "        # Gradient W = delta x A(L - 1)T\n",
    "        gradient_w[-1] = np.dot(delta, acts[-2].T)\n",
    "        \n",
    "        # We now have the L layer with its deltas and gradients claculated.\n",
    "        # Now we iterate over each layer to get the specific deltas\n",
    "        for l in range(2, len(self.layers)):\n",
    "            z = zs[-l] # The minus syntax makes the array to get the farther\n",
    "            \n",
    "            # Use the specified activation function's derivative\n",
    "            activation_derivative = Network.activation_function_hash[self.layers[-l][\"activation\"]][\"func_prime\"]\n",
    "            \n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * activation_derivative(z)\n",
    "            \n",
    "            gradient_b[-l] = delta\n",
    "            \n",
    "            gradient_w[-l] = np.dot(delta, acts[-l - 1].T)\n",
    "            \n",
    "        return (gradient_b, gradient_w)\n",
    "            \n",
    "    def get_costs(self):\n",
    "        return self.loss_history\n",
    "        \n",
    "    def SGD_momentum(self, trainX, trainY, epochs, learn_rate):\n",
    "        \n",
    "        change_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        change_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "    \n",
    "        for epoch in range(0, epochs):\n",
    "            # print(\"Epoch\", epoch + 1)\n",
    "            for X, Y in zip(trainX, trainY):\n",
    "\n",
    "                \"\"\"\n",
    "                The backprop function will propagate the current information\n",
    "                through the network and then calculate the gradients of\n",
    "                each layer. Considering that we have a single bias per layer,\n",
    "                gradient_b will be an array of size L, being L the total number\n",
    "                of layers, excluding the input layer.\n",
    "\n",
    "                gradient_w will be an array of size L too, in which each element\n",
    "                becomes a vector that represents the gradient of all the weights\n",
    "                of a layer.\n",
    "                \"\"\"\n",
    "                (gradient_b, gradient_w) = self.backprop(X, Y)\n",
    "\n",
    "                \"\"\"\n",
    "                Now that we have the gradients of the weights, we want to\n",
    "                store this information for this particular training example in\n",
    "                order to average it later on. The change_b represents the sum of\n",
    "                the changes of all training examples for all biases, and change_w\n",
    "                does the same for all weights.\n",
    "\n",
    "                Later, as if we were trying to get the average of a list, we will\n",
    "                divide this sum by the total amount of examples. This will tell\n",
    "                us the average change of all training examples and allow us to\n",
    "                modify the weights.\n",
    "                \"\"\"\n",
    "                change_b = [(cb + gb)\n",
    "                            for cb, gb in zip(change_b, gradient_b)]\n",
    "                change_w = [(cw + gw)\n",
    "                            for cw, gw in zip(change_w, gradient_w)]\n",
    "\n",
    "            self.weights = [weight - (learn_rate / len(trainX)) * weight_change for weight, weight_change in zip(self.weights, change_w)]\n",
    "            self.biases = [bias -(learn_rate / len(trainX))*bias_change for bias, bias_change in zip(self.biases, change_b)]  \n",
    "            \n",
    "            # Save the loss function on each epoch for future experiments\n",
    "            self.loss_history.append(self.get_total_cost(trainX, trainY))\n",
    "                \n",
    "    \n",
    "# The network should have 2 layers, the first with 32 neurons and the second with 16\n",
    "# You basically will have an architecture like this:\n",
    "# input -> layer 1 (32) -> later 2 (16) -> output (3 neurons with softmax activation function)\n",
    "\n",
    "\"\"\"\n",
    "Bellow here we test our implementation\n",
    "\"\"\"\n",
    "import keras\n",
    "seed(10)\n",
    "T = 100\n",
    "M = 5\n",
    "dataX = np.array([np.random.randn(M, 1) for n in range(0, T)])\n",
    "dataY = [randint(0,2) for n in range(0, T)]\n",
    "dataYOnehot = keras.utils.to_categorical(dataY)\n",
    "network = Network([{\n",
    "    \"neurons\": len(X_train[0]),\n",
    "    \"activation\": \"linear\"\n",
    "}, {\n",
    "    \"neurons\": 32,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 16,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 3,\n",
    "    \"activation\": \"softmax\"\n",
    "}])\n",
    "\n",
    "x = dataX[0]\n",
    "y = dataY[0]\n",
    "\n",
    "\n",
    "network.SGD_momentum(X_train, y_onehot, 700, 0.1)\n",
    "yHat = np.array(network.evaluate(X_train))\n",
    "\n",
    "# Measure accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Tests passed with a score of: \", accuracy_score(y_onehot, yHat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArcAAAHjCAYAAAA5ajcLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEG4BAABQMwi3AAAAqBmEWwAAANSM/w+Pq5tBugIlgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1205c7940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "costs = network.get_costs()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(0, len(costs)), costs, label=\"Loss\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Epocj')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
