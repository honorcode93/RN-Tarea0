{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 4\n",
    "\n",
    "### Funciones de activaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    x = np.clip( x, -200, 200 )\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def softmax_prime(signal):\n",
    "    return softmax(signal) * (1 - softmax(signal))\n",
    "    \n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip( x, -20, 20 )\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    x = np.clip( x, -20, 20 )\n",
    "    upper = np.exp(-x)\n",
    "    lower = ((1 + np.exp(-x))**2)\n",
    "    return upper / lower\n",
    "\n",
    "def relu(x):\n",
    "    # x = np.clip( x, -10, 10 )\n",
    "    x = np.clip( x, -20, 20 )\n",
    "    return np.abs(x) * (x > 0)\n",
    "\n",
    "def relu_prime(x):\n",
    "    # x = np.clip( x, -10, 10 )\n",
    "    x = np.clip( x, -20, 20 )\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Convert the targets to one hot vectors\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Feed-forward con momentum\n",
    "\n",
    "1) $\\delta^L = \\nabla _a C \\odot \\sigma ' (z^L)$, con $\\nabla_ a C = a^L - y$, donde se puede interpretar a $a^L$ como $\\hat{y}$\n",
    "\n",
    "2) $\\delta^l = ((w^{(l + 1)})^T \\delta^{(l+1)}) \\odot \\sigma ' (z^l)$\n",
    "\n",
    "3) $\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_ j$\n",
    "\n",
    "4) $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2, 2, 1, 0, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 0, 1, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 2, 2, 1, 2, 0, 1]\n",
      "Tests passed with a score of:  0.6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAEKCAYAAABJz79KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ac173c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from random import randint, random, seed, uniform, shuffle\n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "warnings.filterwarnings(\"error\")\n",
    "    \n",
    "class Network:\n",
    "    # The layers variable receives a specification with the following format:\n",
    "    # [\n",
    "    #    \"neurons\": n_neurones,\n",
    "    #    \"activation\": \"activation_function\",\n",
    "    # ]\n",
    "    \n",
    "    activation_function_hash = {\n",
    "        \"softmax\": {\n",
    "            \"func\": softmax,\n",
    "            \"func_prime\": softmax_prime,\n",
    "        },\n",
    "        \"sigmoid\": {\n",
    "            \"func\": sigmoid,\n",
    "            \"func_prime\": sigmoid_prime,\n",
    "        },\n",
    "        \n",
    "        \"relu\": {\n",
    "            \"func\": relu,\n",
    "            \"func_prime\": relu_prime,\n",
    "        },\n",
    "        \n",
    "        \"linear\": {\n",
    "            \"func\": linear,\n",
    "            \"func_prime\": 1,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # loss defines the method to calculate cost\n",
    "    # crossentropy or mse\n",
    "    def __init__(self, layers, loss=\"mse\", friction = 1.0, verbose = False):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.loss_history = []\n",
    "        self.mu = friction\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # The first layer is the input layer, and does not have biases\n",
    "        # or weights\n",
    "        self.biases = np.array([np.random.randn(l[\"neurons\"], 1) for l in layers[1:]])\n",
    "        \n",
    "        # The amount of weights depensd on both the amount of neurones on the layer \n",
    "        # and the dimension of the inputs.\n",
    "        self.weights = np.array([np.random.randn(curr_l[\"neurons\"], prev_l[\"neurons\"]) for prev_l, curr_l in zip(layers[:-1], layers[1:])])\n",
    "        self.velocities = np.array([np.random.randn(curr_l[\"neurons\"], prev_l[\"neurons\"]) for prev_l, curr_l in zip(layers[:-1], layers[1:])])\n",
    "        \n",
    "        # self.biases = np.array([np.zeros((l[\"neurons\"], 1)) for l in layers[1:]])\n",
    "        # self.weights = np.array([np.zeros((curr_l[\"neurons\"], prev_l[\"neurons\"])) for prev_l, curr_l in zip(layers[:-1], layers[1:])])\n",
    "        \n",
    "    # Returns final output, activation output at each layer\n",
    "    # and z value at each layer\n",
    "    def forward_propagation(self, data):\n",
    "        output = data\n",
    "        activations = [data]\n",
    "        zs = []\n",
    "        \n",
    "        # We start on the first layer. Note that the input layer\n",
    "        # is pretty much ignored as it does not have activation\n",
    "        # functions or anything like that.\n",
    "        \n",
    "        # First layer is ignored as it only provides dimension of the inputs\n",
    "        for B, W, layer, index in zip(self.biases, self.weights, self.layers[1:], range(0, len(self.biases))):\n",
    "            \n",
    "            # Calculate the Z output. B corresponds to the biases of this layer.\n",
    "            dot = np.dot(W, output)\n",
    "            dot = dot.reshape(len(dot), 1)\n",
    "            Z = dot + B\n",
    "\n",
    "            # Save it for future usage\n",
    "            zs.append(Z)\n",
    "            \n",
    "            # Get the activation function based on the configuratiomn that was given at the construction\n",
    "            # of this network.\n",
    "            activation_string = layer[\"activation\"]\n",
    "            g = Network.activation_function_hash[activation_string][\"func\"]\n",
    "            \n",
    "            # Get the output of the activation function and store it\n",
    "            output = g(Z)\n",
    "            activations.append(output)\n",
    "\n",
    "        # Return a list with all activations an Z values that represent all layers\n",
    "        return activations, zs    \n",
    "    \n",
    "    \n",
    "    # Devuelve el output de la red\n",
    "    def evaluate(self, X):\n",
    "        output = []\n",
    "        for x in X:\n",
    "            (acts, zs) = self.forward_propagation(x)\n",
    "            output.append(acts[-1])\n",
    "            \n",
    "        return np.array(output)\n",
    "    \n",
    "    def classify(self, x):\n",
    "        return np.argmax(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Receives a single sample X with its \n",
    "    # corresponding value Y and returns the \n",
    "    # derivatives for the weights and biases\n",
    "    # We want to get dc/db and dc/dw\n",
    "    \n",
    "    def get_total_cost(self, X, Y):\n",
    "        \n",
    "        n = self.layers[-1][\"neurons\"]\n",
    "        \n",
    "        cost = 0.0\n",
    "        A = self.evaluate(X)\n",
    "        \n",
    "        \n",
    "        if self.loss == \"mse\":\n",
    "            for a, y in zip(A, Y):\n",
    "                cost += 0.5*np.linalg.norm((a.reshape(n, 1) - np.array(y).reshape(n, 1)))**2 / len(A)\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            for a, y in zip(A, Y):\n",
    "                a = a.reshape(n, 1)\n",
    "                y = np.array(y.reshape(n, 1))\n",
    "                \n",
    "                E = 0.0\n",
    "            \n",
    "                cost += np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "        return cost\n",
    "\n",
    "    \n",
    "    def cost_derivative(self, a, y):\n",
    "        if self.loss == \"mse\":\n",
    "            return (a - y)\n",
    "        return None\n",
    "\n",
    "    def get_delta(self, a, z, y, activation_derivative):\n",
    "        # Equation 1 says how this should got, at least for mse\n",
    "        if self.loss == \"mse\":\n",
    "            return (a - y) * activation_derivative(z)\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            return (a - y)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def backprop(self, x, y_true):\n",
    "        # This stores the gradien of each weight and bias. The idea is to calculate\n",
    "        gradient_b = np.array([np.zeros(bias.shape) for bias in self.biases])\n",
    "        gradient_w = np.array([np.zeros(weight.shape) for weight in self.weights])\n",
    "        \n",
    "        # Propagate the input through the network and get all the\n",
    "        # z outputs and activation outputs of all layers\n",
    "        # The first activation corresponds to the first input layer\n",
    "        (acts, zs)  = self.forward_propagation(x)\n",
    "        \n",
    "        # Use the specified activation function's derivative\n",
    "        g_prime = Network.activation_function_hash[self.layers[-1][\"activation\"]][\"func_prime\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # We first calculate the delta value of the output layer, given as\n",
    "        # delta_cost = aL - y\n",
    "        # [ , , , , X] (= -1)\n",
    "        # [ , , , X, ] ( = -2)\n",
    "        # ....\n",
    "        # [X, , , , ,] ( = -L)\n",
    "        n = self.layers[-1][\"neurons\"]\n",
    "        \n",
    "        # We reshape to get the vectors of the output values\n",
    "        # An example would be like this\n",
    "        # z = [[ giberish ],\n",
    "        #      [ giberish ],\n",
    "        #      [ giberish ]]\n",
    "        # \n",
    "        # a = [[ 0.87349 ],\n",
    "        #      [ 0.22223 ],\n",
    "        #      [ 0.01344 ]]\n",
    "        # \n",
    "        # y = [[ 1 ],\n",
    "        #      [ 0 ],\n",
    "        #      [ 0 ]]\n",
    "        # \n",
    "        # Supposedly, the vector a should have norm 1 as we will always use sigmoid\n",
    "        a = acts[-1].reshape(n, 1)\n",
    "        z = zs[-1].reshape(n, 1)\n",
    "        y = y_true.reshape(n, 1)\n",
    "\n",
    "        delta = self.get_delta(a, z, y, g_prime)\n",
    "\n",
    "        # Gradient B = delta\n",
    "        gradient_b[-1] = delta\n",
    "        \n",
    "        # Gradient W = delta x A(L - 1)T\n",
    "        gradient_w[-1] = np.dot(delta, acts[-2].T)\n",
    "\n",
    "        # We now have the L layer with its deltas and gradients calculated.\n",
    "        # Now we iterate over each layer to propagate the gradients \"recursively\"\n",
    "        for layer in range(2, len(self.layers)):\n",
    "            \n",
    "            # Use the specified activation function's derivative\n",
    "            z = zs[-layer]\n",
    "            \n",
    "            g_prime = Network.activation_function_hash[self.layers[-layer][\"activation\"]][\"func_prime\"]\n",
    "            delta = np.dot(self.weights[-layer + 1].T, delta) * g_prime(z)\n",
    "            \n",
    "            gradient_b[-layer] = delta\n",
    "            \n",
    "            # If on first layer, we reshape the input data\n",
    "            if (layer == len(self.layers) - 1):\n",
    "                \n",
    "                dim = len(acts[-layer - 1])\n",
    "                \n",
    "                gradient_w[-layer] = np.dot(delta, np.array(acts[-layer - 1]).reshape(dim, 1).T)\n",
    "            else:\n",
    "                gradient_w[-layer] = np.dot(delta, acts[-layer - 1].T)\n",
    "            \n",
    "        return (gradient_b, gradient_w)\n",
    "            \n",
    "    def get_costs(self):\n",
    "        return self.loss_history\n",
    "    \n",
    "    def SGD(self, trainX, trainY, epochs, lr):\n",
    "        for e in range(0, epochs):\n",
    "            nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            for x, y in zip(trainX, trainY):\n",
    "                delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "                nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "                nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "                \n",
    "                \n",
    "            self.velocities = [self.mu*v-(lr/len(trainX))*nw\n",
    "                            for v, nw in zip(self.velocities, nabla_w)]\n",
    "            self.weights = [w + v\n",
    "                            for w, v in zip(self.weights, self.velocities)]\n",
    "            self.biases = [b-(lr/len(trainX))*nb\n",
    "                           for b, nb in zip(self.biases, nabla_b)]\n",
    "            \n",
    "            cost = self.get_total_cost(trainX, trainY)\n",
    "            if self.verbose:\n",
    "                print(\"Epoch \", e + 1, \"/\", epochs, \":\", cost)\n",
    "            self.loss_history.append(cost)\n",
    "                \n",
    "    \n",
    "# The network should have 2 layers, the first with 32 neurons and the second with 16\n",
    "# You basically will have an architecture like this:\n",
    "# input -> layer 1 (32) -> later 2 (16) -> output (3 neurons with softmax activation function)\n",
    "\n",
    "\"\"\"\n",
    "Bellow here we test our implementation\n",
    "\"\"\"\n",
    "import keras\n",
    "T = 100\n",
    "M = 5\n",
    "dataX = np.array([np.random.randn(M, 1) for n in range(0, T)])\n",
    "dataY = [randint(0,2) for n in range(0, T)]\n",
    "dataYOnehot = keras.utils.to_categorical(dataY)\n",
    "network = Network([{\n",
    "    \"neurons\": len(X_train[0]),\n",
    "    \"activation\": \"linear\"\n",
    "}, {\n",
    "    \"neurons\": 32,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 16,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 3,\n",
    "    \"activation\": \"softmax\"\n",
    "}], loss=\"mse\", friction=1.0, verbose=False)\n",
    "\n",
    "x = dataX[0]\n",
    "y = dataY[0]\n",
    "\n",
    "\n",
    "network.SGD(X_train, y_onehot, 2000, 0.1)\n",
    "yHat = np.array(network.evaluate(X_train))\n",
    "\n",
    "yHatHot = []\n",
    "\n",
    "for yi in yHat:\n",
    "    yHatHot.append(network.classify(yi))\n",
    "\n",
    "# Measure classification accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(yHatHot)\n",
    "print(\"Tests passed with a score of: \", accuracy_score(y_train, yHatHot))\n",
    "\n",
    "\n",
    "# Graph the loss function \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "costs = network.get_costs()\n",
    "\n",
    "plt.plot(range(0, len(costs)), costs, label=\"Loss\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay una mejora significativa en el tiempo de ejecuciÃ³n del algoritmo, ni en la calidad de la soluciÃ³n a la que converge. TeÃ³ricamente, la utilizaciÃ³n de momentum deberÃ­a acelerar la convergencia en comparaciÃ³n a Gradient Descent normal. Se puede explicar la falta de mejora por las restricciones impuestas a las funciones de activaciÃ³n para evitar el overflow numÃ©rico. El mejor accuracy obtendio con 2000 epochs y sigmoid corresponde a 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
