{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 4\n",
    "\n",
    "### Funciones de activaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def softmax_prime(x):\n",
    "    upper = np.exp(-x)\n",
    "    down = np.power(1 + np.exp(-x), 2)\n",
    "    return upper / down\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return np.exp(-x)/((1+np.exp(-x))**2)\n",
    "\n",
    "def relu(x):\n",
    "    return np.abs(x) * (x > 0)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Feed-forward con momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed with a score of:  0.36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7500000000000006,\n",
       " 0.7500000000000006,\n",
       " 0.7400000000000005,\n",
       " 0.7400000000000005,\n",
       " 0.7300000000000005,\n",
       " 0.7300000000000005,\n",
       " 0.7600000000000006,\n",
       " 0.7500000000000006,\n",
       " 0.7600000000000006,\n",
       " 0.7700000000000006,\n",
       " 0.7400000000000005,\n",
       " 0.7200000000000005,\n",
       " 0.7100000000000005,\n",
       " 0.6700000000000005,\n",
       " 0.6700000000000005,\n",
       " 0.6500000000000005,\n",
       " 0.6600000000000005,\n",
       " 0.6700000000000005,\n",
       " 0.6500000000000005,\n",
       " 0.6600000000000005,\n",
       " 0.6700000000000005,\n",
       " 0.6600000000000005,\n",
       " 0.6500000000000005,\n",
       " 0.6200000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6100000000000004,\n",
       " 0.6000000000000004,\n",
       " 0.6000000000000004,\n",
       " 0.6000000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5700000000000004,\n",
       " 0.5700000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.6000000000000004,\n",
       " 0.6000000000000004,\n",
       " 0.6000000000000004,\n",
       " 0.6000000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.6000000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5800000000000004,\n",
       " 0.5900000000000004,\n",
       " 0.6100000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6700000000000005,\n",
       " 0.6600000000000005,\n",
       " 0.6600000000000005,\n",
       " 0.6600000000000005,\n",
       " 0.6500000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6500000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6500000000000005,\n",
       " 0.6200000000000004,\n",
       " 0.6500000000000005,\n",
       " 0.6700000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6500000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6500000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6200000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6300000000000004,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005,\n",
       " 0.6400000000000005]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randint, random, seed, uniform\n",
    "import keras\n",
    "    \n",
    "class Network:\n",
    "    # The layers variable receives a specification with the following format:\n",
    "    # [\n",
    "    #    \"neurons\": n_neurones,\n",
    "    #    \"activation\": \"activation_function\",\n",
    "    # ]\n",
    "    \n",
    "    activation_function_hash = {\n",
    "        \"softmax\": {\n",
    "            \"func\": softmax,\n",
    "            \"func_prime\": softmax_prime,\n",
    "        },\n",
    "        \"sigmoid\": {\n",
    "            \"func\": sigmoid,\n",
    "            \"func_prime\": sigmoid_prime,\n",
    "        },\n",
    "        \n",
    "        \"relu\": {\n",
    "            \"func\": relu,\n",
    "            \"func_prime\": relu_prime,\n",
    "        },\n",
    "        \n",
    "        \"linear\": {\n",
    "            \"func\": linear,\n",
    "            \"func_prime\": 1,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # loss defines the method to calculate cost\n",
    "    # crossentropy or mse\n",
    "    def __init__(self, layers, loss=\"mse\", mu = 1.0):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.loss_history = []\n",
    "        self.mu = mu\n",
    "        \n",
    "        # The first layer is the input layer, and does not have biases\n",
    "        # or weights\n",
    "        self.biases = np.array([np.random.randn(l[\"neurons\"], 1) for l in layers[1:]])\n",
    "        \n",
    "        # The amount of weights depensd on both the amount of neurones on the layer \n",
    "        # and the dimension of the inputs.\n",
    "        self.weights = [np.random.randn(curr_l[\"neurons\"], prev_l[\"neurons\"]) for prev_l, curr_l in zip(layers[:-1], layers[1:])]\n",
    "        self.velocities = [np.zeros(curr_l[\"neurons\"], prev_l[\"neurons\"]) for prev_l, curr_l in zip(layers[:-1], layers[1:])]\n",
    "\n",
    "    # Returns final output, activation output at each layer\n",
    "    # and z value at each layer\n",
    "    def forward_propagation(self, data):\n",
    "        output = data\n",
    "        activations = [data.reshape(len(data), 1)]\n",
    "        zs = []\n",
    "        \n",
    "        # We start on the first layer. Note that the input layer\n",
    "        # is pretty much ignored as it does not have activation\n",
    "        # functions or anything like that.\n",
    "        for B, W, layer, index in zip(self.biases, self.weights, self.layers, range(0, len(self.biases))):\n",
    "            dot = np.dot(W, output)\n",
    "            dot = dot.reshape(len(dot), 1)\n",
    "            Z = dot + B\n",
    "\n",
    "            zs.append(Z)\n",
    "            activation_string = layer[\"activation\"]\n",
    "            g = Network.activation_function_hash[activation_string][\"func\"]\n",
    "            output = g(Z)\n",
    "            \n",
    "            \n",
    "            \n",
    "            activations.append(output)\n",
    "\n",
    "        return activations, zs    \n",
    "    \n",
    "    \n",
    "    # Devuelve el output de la red\n",
    "    def evaluate(self, X):\n",
    "        output = []\n",
    "        for x in X:\n",
    "            (acts, zs) = self.forward_propagation(x)\n",
    "            output.append(self.classify(acts[len(self.biases)]))\n",
    "            \n",
    "        return np.array(output)\n",
    "    \n",
    "    def classify(self, x):\n",
    "        chosen_class = 2 ** np.argmax(x)\n",
    "        total_classes = self.layers[-1][\"neurons\"]\n",
    "        return (((chosen_class & (1 << np.arange(total_classes)))) > 0).astype(int)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Receives a single sample X with its \n",
    "    # corresponding value Y and returns the \n",
    "    # derivatives for the weights and biases\n",
    "    # We want to get dc/db and dc/dw\n",
    "    \n",
    "    def get_total_cost(self, X, Y):\n",
    "        \n",
    "        n = self.layers[-1][\"neurons\"]\n",
    "        \n",
    "        cost = 0.0\n",
    "        A = self.evaluate(X)\n",
    "        \n",
    "        \n",
    "        if self.loss == \"mse\":\n",
    "            for a, y in zip(A, Y):\n",
    "                cost += 0.5*np.linalg.norm(a.reshape(n, 1) - np.array(y).reshape(n, 1))**2 / len(A)\n",
    "            return cost\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            for a, y in zip(A, Y):\n",
    "                a = a.reshape(n, 1)\n",
    "                y = np.array(y.reshape(n, 1))\n",
    "                cost += np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) / len(A)\n",
    "        return cost\n",
    "    def get_delta(self, a, z, y, activation_derivative):\n",
    "        # Equation 1 says how this should got, at least for mse\n",
    "        if self.loss == \"mse\":\n",
    "            return (a - y) * activation_derivative(z)\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            return (a - y)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        \n",
    "        # Initialize the gradient\n",
    "        gradient_b = np.array([np.zeros(bias.shape) for bias in self.biases])\n",
    "        gradient_w = np.array([np.zeros(weight.shape) for weight in self.weights])\n",
    "        \n",
    "        # Propagate the input through the network and get all the\n",
    "        # z outputs and activation outputs of all layers\n",
    "        # The first activation corresponds to the first input layer\n",
    "        (acts, zs)  = self.forward_propagation(x)\n",
    "        \n",
    "        # Use the specified activation function's derivative\n",
    "        activation_derivative = Network.activation_function_hash[self.layers[-1][\"activation\"]][\"func_prime\"]\n",
    "        \n",
    "        # We first calculate the delta value of the output layer, given as\n",
    "        # delta_cost = aL - y\n",
    "        # [ , , , , X] (= -1)\n",
    "        # [ , , , X, ] ( = -2)\n",
    "        # ....\n",
    "        # [X, , , , ,] ( = -L)\n",
    "        \n",
    "        yHat = np.array(self.classify(acts[-1]))\n",
    "\n",
    "        delta = self.get_delta(yHat.reshape(3, 1), zs[-1], y.reshape(3, 1), activation_derivative)\n",
    "        \n",
    "        # \n",
    "        # Gradient B = delta\n",
    "        gradient_b[-1] = delta\n",
    "        \n",
    "        # Gradient W = delta x A(L - 1)T\n",
    "        gradient_w[-1] = np.dot(delta, acts[-2].T)\n",
    "        \n",
    "        # We now have the L layer with its deltas and gradients claculated.\n",
    "        # Now we iterate over each layer to get the specific deltas\n",
    "        for l in range(2, len(self.layers)):\n",
    "            z = zs[-l] # The minus syntax makes the array to get the farther\n",
    "            \n",
    "            # Use the specified activation function's derivative\n",
    "            activation_derivative = Network.activation_function_hash[self.layers[-l][\"activation\"]][\"func_prime\"]\n",
    "            \n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * activation_derivative(z)\n",
    "            \n",
    "            gradient_b[-l] = delta\n",
    "            \n",
    "            gradient_w[-l] = np.dot(delta, acts[-l - 1].T)\n",
    "            \n",
    "        return (gradient_b, gradient_w)\n",
    "            \n",
    "    def get_costs(self):\n",
    "        return self.loss_history\n",
    "        \n",
    "    def SGD_momentum(self, trainX, trainY, epochs, learn_rate):\n",
    "        \n",
    "        change_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        change_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "    \n",
    "        for epoch in range(0, epochs):\n",
    "            # print(\"Epoch\", epoch + 1)\n",
    "            for X, Y in zip(trainX, trainY):\n",
    "\n",
    "                \"\"\"\n",
    "                The backprop function will propagate the current information\n",
    "                through the network and then calculate the gradients of\n",
    "                each layer. Considering that we have a single bias per layer,\n",
    "                gradient_b will be an array of size L, being L the total number\n",
    "                of layers, excluding the input layer.\n",
    "\n",
    "                gradient_w will be an array of size L too, in which each element\n",
    "                becomes a vector that represents the gradient of all the weights\n",
    "                of a layer.\n",
    "                \"\"\"\n",
    "                (gradient_b, gradient_w) = self.backprop(X, Y)\n",
    "\n",
    "                \"\"\"\n",
    "                Now that we have the gradients of the weights, we want to\n",
    "                store this information for this particular training example in\n",
    "                order to average it later on. The change_b represents the sum of\n",
    "                the changes of all training examples for all biases, and change_w\n",
    "                does the same for all weights.\n",
    "\n",
    "                Later, as if we were trying to get the average of a list, we will\n",
    "                divide this sum by the total amount of examples. This will tell\n",
    "                us the average change of all training examples and allow us to\n",
    "                modify the weights.\n",
    "                \"\"\"\n",
    "                change_b = [(cb + gb)\n",
    "                            for cb, gb in zip(change_b, gradient_b)]\n",
    "                change_w = [(cw + gw)\n",
    "                            for cw, gw in zip(change_w, gradient_w)]\n",
    "\n",
    "            self.weights = [weight - (learn_rate / len(trainX)) * weight_change for weight, weight_change in zip(self.weights, change_w)]\n",
    "            self.biases = [bias -(learn_rate / len(trainX))*bias_change for bias, bias_change in zip(self.biases, change_b)]  \n",
    "            \n",
    "            # Save the loss function on each epoch for future experiments\n",
    "            self.loss_history.append(self.get_total_cost(trainX, trainY))\n",
    "                \n",
    "    \n",
    "# The network should have 2 layers, the first with 32 neurons and the second with 16\n",
    "# You basically will have an architecture like this:\n",
    "# input -> layer 1 (32) -> later 2 (16) -> output (3 neurons with softmax activation function)\n",
    "\n",
    "\"\"\"\n",
    "Bellow here we test our implementation\n",
    "\"\"\"\n",
    "import keras\n",
    "seed(10)\n",
    "T = 100\n",
    "M = 5\n",
    "dataX = np.array([np.random.randn(M, 1) for n in range(0, T)])\n",
    "dataY = [randint(0,2) for n in range(0, T)]\n",
    "dataYOnehot = keras.utils.to_categorical(dataY)\n",
    "network = Network([{\n",
    "    \"neurons\": M,\n",
    "    \"activation\": \"linear\"\n",
    "}, {\n",
    "    \"neurons\": 32,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 16,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 3,\n",
    "    \"activation\": \"softmax\"\n",
    "}])\n",
    "\n",
    "x = dataX[0]\n",
    "y = dataY[0]\n",
    "\n",
    "\n",
    "network.SGD_momentum(dataX, dataYOnehot, 300, 0.1)\n",
    "yHat = np.array(network.evaluate(dataX))\n",
    "\n",
    "# Measure accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Tests passed with a score of: \", accuracy_score(dataYOnehot, yHat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
