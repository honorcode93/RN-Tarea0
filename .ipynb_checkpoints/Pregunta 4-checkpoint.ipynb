{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 4\n",
    "\n",
    "### Funciones de activaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    # signal = np.clip( signal, -10, 10 )\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def softmax_prime(signal):\n",
    "    #signal = np.clip( signal, -10, 10 )\n",
    "    J = - signal[..., None] * signal[:, None, :] # off-diagonal Jacobian\n",
    "    iy, ix = np.diag_indices_from(J[0])\n",
    "    J[:, iy, ix] = signal * (1. - signal) # diagonal\n",
    "    return J.sum(axis=1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # x = np.clip( x, -10, 10 )\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    # x = np.clip( x, -10, 10 )\n",
    "    upper = np.exp(-x)\n",
    "    lower = ((1 + np.exp(-x))**2)\n",
    "    return upper / lower\n",
    "\n",
    "def relu(x):\n",
    "    # x = np.clip( x, -10, 10 )\n",
    "    return np.abs(x) * (x > 0)\n",
    "\n",
    "def relu_prime(x):\n",
    "    # x = np.clip( x, -10, 10 )\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Convert the targets to one hot vectors\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Feed-forward con momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Biases in last layer [[ 2.72463641]\n",
      " [ 0.66027217]\n",
      " [-0.36563795]]\n",
      "Weights in last layer [[-0.5093321   2.12695603  2.54337198 -0.20677223  0.68388174  2.2856039\n",
      "   2.5322364   1.2776382 ]\n",
      " [-1.41670518 -0.88065375 -0.55883765  0.48388297 -0.34998199  1.49342404\n",
      "  -0.41652378 -0.91872006]\n",
      " [-0.73947066 -0.61851868 -0.89910109  0.15115259  0.19549735  1.2517426\n",
      "  -0.05278492 -0.54703883]]\n",
      "Epoch 2\n",
      "Biases in last layer [[ 9.62951777]\n",
      " [ 0.57779823]\n",
      " [-0.43345355]]\n",
      "Weights in last layer [[ 0.20169431  4.78250125  6.80109145  0.09630087  3.2029219   6.85145513\n",
      "   7.80299311  6.5685765 ]\n",
      " [-1.46619102 -0.88257132 -0.64093699  0.47635704 -0.37988945  1.49060353\n",
      "  -0.49752751 -0.99738609]\n",
      " [-0.77767868 -0.62210867 -0.96171769  0.14601596  0.17706068  1.24459429\n",
      "  -0.11712411 -0.59983096]]\n",
      "Epoch 3\n",
      "Biases in last layer [[128.34830671]\n",
      " [  0.46815034]\n",
      " [ -0.5266795 ]]\n",
      "Weights in last layer [[  2.57862776 102.85184582 106.6684535   10.27731104 113.56456624\n",
      "  119.48165433 123.02778102 113.38711467]\n",
      " [ -1.51921042  -0.90840638  -0.73482492   0.46673484  -0.42486585\n",
      "    1.47520732  -0.59885804  -1.10281792]\n",
      " [ -0.81610661  -0.65198053  -1.04893104   0.14069831   0.13418451\n",
      "    1.21313883  -0.20728203  -0.67794153]]\n",
      "Epoch 4\n",
      "Biases in last layer [[ 3.74602557e+04]\n",
      " [ 2.55448099e-01]\n",
      " [-7.45428112e-01]]\n",
      "Weights in last layer [[ 1.45788061e+04  3.70369834e+04  3.66313630e+04  3.69596891e+04\n",
      "   3.61141283e+04  3.67052711e+04  2.86118106e+04  3.66996902e+04]\n",
      " [-1.66052431e+00 -1.03197477e+00 -9.25368211e-01  3.59034629e-01\n",
      "  -5.62083360e-01  1.36032810e+00 -7.99929839e-01 -1.30999765e+00]\n",
      " [-9.48907704e-01 -8.01241120e-01 -1.25641922e+00  1.58481295e-02\n",
      "  -1.41642587e-02  1.07659375e+00 -3.89228944e-01 -8.80774607e-01]]\n",
      "Infinity detected in weight [nan nan nan nan] [[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unhealthy network",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-75a41b614693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0myHat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-75a41b614693>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, trainX, trainY, epochs, learn_rate)\u001b[0m\n\u001b[1;32m    254\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Infinity detected in weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unhealthy network'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_biases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unhealthy network"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from random import randint, random, seed, uniform, shuffle\n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "class Network:\n",
    "    # The layers variable receives a specification with the following format:\n",
    "    # [\n",
    "    #    \"neurons\": n_neurones,\n",
    "    #    \"activation\": \"activation_function\",\n",
    "    # ]\n",
    "    \n",
    "    activation_function_hash = {\n",
    "        \"softmax\": {\n",
    "            \"func\": softmax,\n",
    "            \"func_prime\": softmax_prime,\n",
    "        },\n",
    "        \"sigmoid\": {\n",
    "            \"func\": sigmoid,\n",
    "            \"func_prime\": sigmoid_prime,\n",
    "        },\n",
    "        \n",
    "        \"relu\": {\n",
    "            \"func\": relu,\n",
    "            \"func_prime\": relu_prime,\n",
    "        },\n",
    "        \n",
    "        \"linear\": {\n",
    "            \"func\": linear,\n",
    "            \"func_prime\": 1,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # loss defines the method to calculate cost\n",
    "    # crossentropy or mse\n",
    "    def __init__(self, layers, loss=\"mse\", mu = 1.0):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.loss_history = []\n",
    "        self.mu = mu\n",
    "        \n",
    "        # The first layer is the input layer, and does not have biases\n",
    "        # or weights\n",
    "        self.biases = np.array([np.random.randn(l[\"neurons\"], 1) for l in layers[1:]])\n",
    "        \n",
    "        # The amount of weights depensd on both the amount of neurones on the layer \n",
    "        # and the dimension of the inputs.\n",
    "        self.weights = np.array([np.random.randn(curr_l[\"neurons\"], prev_l[\"neurons\"]) for prev_l, curr_l in zip(layers[:-1], layers[1:])])\n",
    "        \n",
    "        # self.biases = np.array([np.zeros((l[\"neurons\"], 1)) for l in layers[1:]])\n",
    "        # self.weights = np.array([np.zeros((curr_l[\"neurons\"], prev_l[\"neurons\"])) for prev_l, curr_l in zip(layers[:-1], layers[1:])])\n",
    "        \n",
    "    # Returns final output, activation output at each layer\n",
    "    # and z value at each layer\n",
    "    def forward_propagation(self, data):\n",
    "        output = data\n",
    "        activations = [data]\n",
    "        zs = []\n",
    "        \n",
    "        # We start on the first layer. Note that the input layer\n",
    "        # is pretty much ignored as it does not have activation\n",
    "        # functions or anything like that.\n",
    "        for B, W, layer, index in zip(self.biases, self.weights, self.layers, range(0, len(self.biases))):\n",
    "            dot = np.dot(W, output)\n",
    "            dot = dot.reshape(len(dot), 1)\n",
    "            Z = dot + B\n",
    "\n",
    "            zs.append(Z)\n",
    "            activation_string = layer[\"activation\"]\n",
    "            g = Network.activation_function_hash[activation_string][\"func\"]\n",
    "            output = g(Z)\n",
    "            activations.append(output)\n",
    "\n",
    "        return activations, zs    \n",
    "    \n",
    "    \n",
    "    # Devuelve el output de la red\n",
    "    def evaluate(self, X):\n",
    "        output = []\n",
    "        for x in X:\n",
    "            (acts, zs) = self.forward_propagation(x)\n",
    "            output.append(self.classify(acts[len(self.biases)]))\n",
    "            \n",
    "        return np.array(output)\n",
    "    \n",
    "    def classify(self, x):\n",
    "        chosen_class = 2 ** np.argmax(x)\n",
    "        total_classes = self.layers[-1][\"neurons\"]\n",
    "        return (((chosen_class & (1 << np.arange(total_classes)))) > 0).astype(int)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Receives a single sample X with its \n",
    "    # corresponding value Y and returns the \n",
    "    # derivatives for the weights and biases\n",
    "    # We want to get dc/db and dc/dw\n",
    "    \n",
    "    def get_total_cost(self, X, Y):\n",
    "        \n",
    "        n = self.layers[-1][\"neurons\"]\n",
    "        \n",
    "        cost = 0.0\n",
    "        A = self.evaluate(X)\n",
    "        \n",
    "        if self.loss == \"mse\":\n",
    "            for a, y in zip(A, Y):\n",
    "                cost += 0.5*np.linalg.norm((a.reshape(n, 1) - np.array(y).reshape(n, 1)))**2 / len(A)\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            for a, y in zip(A, Y):\n",
    "                a = a.reshape(n, 1)\n",
    "                y = np.array(y.reshape(n, 1))\n",
    "                cost += np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) / len(A)\n",
    "        return cost\n",
    "\n",
    "    def get_delta(self, a, z, y, activation_derivative):\n",
    "        # Equation 1 says how this should got, at least for mse\n",
    "        if self.loss == \"mse\":\n",
    "            return (a - y) * activation_derivative(z)\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            return (a - y)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        # Initialize the gradient\n",
    "        gradient_b = np.array([np.zeros(bias.shape) for bias in self.biases])\n",
    "        gradient_w = np.array([np.zeros(weight.shape) for weight in self.weights])\n",
    "        \n",
    "        # Propagate the input through the network and get all the\n",
    "        # z outputs and activation outputs of all layers\n",
    "        # The first activation corresponds to the first input layer\n",
    "        (acts, zs)  = self.forward_propagation(x)\n",
    "        \n",
    "        # Use the specified activation function's derivative\n",
    "        activation_derivative = Network.activation_function_hash[self.layers[-1][\"activation\"]][\"func_prime\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # We first calculate the delta value of the output layer, given as\n",
    "        # delta_cost = aL - y\n",
    "        # [ , , , , X] (= -1)\n",
    "        # [ , , , X, ] ( = -2)\n",
    "        # ....\n",
    "        # [X, , , , ,] ( = -L)\n",
    "        n = self.layers[-1][\"neurons\"]\n",
    "\n",
    "        delta = self.get_delta(acts[-1].reshape(n, 1), zs[-1].reshape(n, 1), y.reshape(n, 1), activation_derivative)\n",
    "        # \n",
    "        # Gradient B = delta\n",
    "        gradient_b[-1] = delta\n",
    "        \n",
    "        # Gradient W = delta x A(L - 1)T\n",
    "        gradient_w[-1] = np.dot(delta, acts[-2].T)\n",
    "\n",
    "        # We now have the L layer with its deltas and gradients claculated.\n",
    "        # Now we iterate over each layer to get the specific deltas\n",
    "        for l in range(2, len(self.layers)):\n",
    "            \n",
    "            # Use the specified activation function's derivative\n",
    "            activation_derivative = Network.activation_function_hash[self.layers[-l][\"activation\"]][\"func_prime\"]\n",
    "\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * activation_derivative(zs[-l])\n",
    "            \n",
    "            gradient_b[-l] = delta\n",
    "            \n",
    "            if (l == len(self.layers) - 1):\n",
    "                \n",
    "                dim = len(acts[-l - 1])\n",
    "                \n",
    "                gradient_w[-l] = np.dot(delta, np.array(acts[-l - 1]).reshape(dim, 1).T)\n",
    "            else:\n",
    "                gradient_w[-l] = np.dot(delta, acts[-l - 1].T)\n",
    "            \n",
    "        return (gradient_b, gradient_w)\n",
    "            \n",
    "    def get_costs(self):\n",
    "        return self.loss_history\n",
    "        \n",
    "    def SGD(self, trainX, trainY, epochs, learn_rate):\n",
    "        \n",
    "        change_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        change_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "    \n",
    "        for epoch in range(0, epochs):\n",
    "            np.random.shuffle(trainX)\n",
    "        \n",
    "            for X, Y in zip(trainX, trainY):\n",
    "\n",
    "                \"\"\"\n",
    "                The backprop function will propagate the current information\n",
    "                through the network and then calculate the gradients of\n",
    "                each layer. Considering that we have a single bias per layer,\n",
    "                gradient_b will be an array of size L, being L the total number\n",
    "                of layers, excluding the input layer.\n",
    "\n",
    "                gradient_w will be an array of size L too, in which each element\n",
    "                becomes a vector that represents the gradient of all the weights\n",
    "                of a layer.\n",
    "                \"\"\"\n",
    "                (gradient_b, gradient_w) = self.backprop(X, Y)\n",
    "\n",
    "                \"\"\"\n",
    "                Now that we have the gradients of the weights, we want to\n",
    "                store this information for this particular training example in\n",
    "                order to average it later on. The change_b represents the sum of\n",
    "                the changes of all training examples for all biases, and change_w\n",
    "                does the same for all weights.\n",
    "\n",
    "                Later, as if we were trying to get the average of a list, we will\n",
    "                divide this sum by the total amount of examples. This will tell\n",
    "                us the average change of all training examples and allow us to\n",
    "                modify the weights.\n",
    "                \"\"\"\n",
    "                \n",
    "                for cb in change_b:\n",
    "                    if(np.isinf(cb).any()):\n",
    "                        print(\"Invalid change_b\", epoch, cb)\n",
    "                        raise ValueError('Unhealthy network')\n",
    "                    \n",
    "                for cw in change_w:\n",
    "                    if(np.isinf(cw).any()):\n",
    "                        print(\"Invalid change_w\", epoch, cw)\n",
    "                        raise ValueError('Unhealthy network')\n",
    "                      \n",
    "                    \n",
    "                for gb in gradient_b:\n",
    "                    if(np.isinf(gb).any()):\n",
    "                        print(\"Invalid gradient_b\", epoch, gb)\n",
    "                        raise ValueError('Unhealthy network')\n",
    "\n",
    "                for gw in gradient_w:\n",
    "                    if(np.isinf(gw).any()):\n",
    "                        print(\"Invalid gradient_w\", epoch, gw)\n",
    "                        raise ValueError('Unhealthy network')\n",
    "                \n",
    "                change_b = [(cb + gb)\n",
    "                            for cb, gb in zip(change_b, gradient_b)]\n",
    "                change_w = [(cw + gw)\n",
    "                            for cw, gw in zip(change_w, gradient_w)]\n",
    "\n",
    "\n",
    "            self.weights = [weight - (learn_rate / len(trainX)) * weight_change for weight, weight_change in zip(self.weights, change_w)]\n",
    "            self.biases = [bias -(learn_rate / len(trainX)) * bias_change for bias, bias_change in zip(self.biases, change_b)]  \n",
    "            \n",
    "            # Look for unhealthy values\n",
    "            \n",
    "            \n",
    "            for layer_biases, layer_weights in zip(self.biases, self.weights):\n",
    "                for weight in layer_weights:\n",
    "                    if (np.isinf(weight).any() or np.isnan(weight).any()):\n",
    "                        print(\"Infinity detected in weight\", weight, layer_weights)\n",
    "                        raise ValueError('Unhealthy network')\n",
    "                for bias in layer_biases:\n",
    "                    if (np.isinf(bias) or np.isnan(bias)):\n",
    "                        print(\"Infinity detected in bias\", bias, layer_biases)\n",
    "                        raise ValueError('Unhealthy network')\n",
    "                        \n",
    "            print(\"Epoch\", epoch + 1)\n",
    "            print(\"Biases in last layer\", self.biases[-1])\n",
    "            print(\"Weights in last layer\", self.weights[-1])\n",
    "                \n",
    "            \n",
    "            cost = self.get_total_cost(trainX, trainY)\n",
    "            # print(\"Epoch\", epoch + 1, \"(\", cost,\")\")\n",
    "            # Save the loss function on each epoch for future experiments\n",
    "            self.loss_history.append(cost)\n",
    "                \n",
    "    \n",
    "# The network should have 2 layers, the first with 32 neurons and the second with 16\n",
    "# You basically will have an architecture like this:\n",
    "# input -> layer 1 (32) -> later 2 (16) -> output (3 neurons with softmax activation function)\n",
    "\n",
    "\"\"\"\n",
    "Bellow here we test our implementation\n",
    "\"\"\"\n",
    "import keras\n",
    "T = 100\n",
    "M = 5\n",
    "dataX = np.array([np.random.randn(M, 1) for n in range(0, T)])\n",
    "dataY = [randint(0,2) for n in range(0, T)]\n",
    "dataYOnehot = keras.utils.to_categorical(dataY)\n",
    "network = Network([{\n",
    "    \"neurons\": len(X_train[0]),\n",
    "    \"activation\": \"linear\"\n",
    "}, {\n",
    "    \"neurons\": 16,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 8,\n",
    "    \"activation\": \"sigmoid\"\n",
    "}, {\n",
    "    \"neurons\": 3,\n",
    "    \"activation\": \"softmax\"\n",
    "}])\n",
    "\n",
    "x = dataX[0]\n",
    "y = dataY[0]\n",
    "\n",
    "\n",
    "network.SGD(X_train, y_onehot, 10, 0.1)\n",
    "yHat = np.array(network.evaluate(X_train))\n",
    "\n",
    "# Measure classification accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Tests passed with a score of: \", accuracy_score(y_onehot, yHat))\n",
    "\n",
    "\n",
    "# Graph the loss function \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "costs = network.get_costs()\n",
    "\n",
    "plt.plot(range(0, len(costs)), costs, label=\"Loss\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
